{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules\n",
    "\n",
    "> This package contains the modules that make up the [FastSpeech](https://arxiv.org/abs/1905.09263) architecture\n",
    "![](../assets/fastspeech-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import math\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from fastspeech.visualize import show_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def get_shape(*args: tensor): return tuple(map(lambda x: x.shape, [*args]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "vocab_sz = 50\n",
    "n_hidden = 48\n",
    "filter_sz = 64\n",
    "n_heads = 2\n",
    "bs = 16\n",
    "seq_len = 18\n",
    "out_shape = [bs, seq_len, n_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "sample_batch = torch.randint(vocab_sz, (bs, seq_len))\n",
    "sample_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phoneme embedding\n",
    "> The first module of the fastspeech architecture is the input embeddings where they embed the input phonemes in to the models hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18, 48])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(vocab_sz, n_hidden)\n",
    "samples_embedded = embedding(sample_batch)\n",
    "samples_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(out_shape, samples_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positional embedding\n",
    "> After the embedding layer in the fastspeech model it inputs positional embedding to allow the model to have information on the positons of inputs. The positional embedding used in the [FastSpeech](https://arxiv.org/abs/1905.09263) paper is the function described in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_positional_embeddings(seq_len, # The length of the sequence\n",
    "                              d_model, # The hidden dimension of the model\n",
    "                              device: torch.device =None): # Device you want to use\n",
    "    pos = torch.arange(d_model, device=device)[None, :]\n",
    "    i = torch.arange(seq_len, device=device)[:, None]\n",
    "    angle = pos / torch.pow(10000, 2 * i / d_model)\n",
    "    pos_emb = torch.zeros(angle.shape, device=device)\n",
    "    pos_emb[0::2,:], pos_emb[1::2,:] = angle[0::2,:].sin(), angle[1::2,:].cos()\n",
    "    return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 48])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb = get_positional_embeddings(seq_len, n_hidden)\n",
    "pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(pos_emb.shape, [seq_len, n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18, 48])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = samples_embedded + pos_emb\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(inp.shape, out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        nn.init.xavier_uniform_(self.weight, nn.init.calculate_gain('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv1D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mConv1D\u001b[49m(seq_len, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m)(inp)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Conv1D' is not defined"
     ]
    }
   ],
   "source": [
    "Conv1d(seq_len, 10, 1)(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        nn.init.xavier_uniform_(self.weight, nn.init.calculate_gain('linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed-forward transformer\n",
    "> This component of the model is the engine of the model. It is what will be used to make up the phoneme encoder and mel spectrogram decoder. It consists of a Multi-Head Attention block and a Conv Network. An additional note from the paper is that prior to the addition of residual inputs and the normalization. ![Scaled Dot Product Attention and Multi-Head Attention](../assets/multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''The Multi-Head Attention component comes from the \n",
    "    [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper. \n",
    "    For the purpose of simplicity we combine the two parts of the Multi-Headed Attention \n",
    "    into one module'''\n",
    "    def __init__(self, \n",
    "                 ni: int, # The input dimension \n",
    "                 nh: int): # The number of attention heads\n",
    "        ''''''\n",
    "        super().__init__()\n",
    "        self.nh = nh\n",
    "        self.scale = math.sqrt(ni / nh)\n",
    "        self.kqv = Linear(ni, ni*3)\n",
    "        self.proj = Linear(ni, ni)\n",
    "        \n",
    "    def forward(self, inp: tensor):\n",
    "        x = self.kqv(inp)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nh)\n",
    "        \n",
    "        Q, K, V = torch.chunk(x, 3, dim=-1)\n",
    "        x = F.softmax(Q @ K.transpose(1,2) / self.scale, dim=-1) @ V\n",
    "    \n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nh)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiHeadAttention(n_hidden, n_hidden)\n",
    "ho = attention(inp)\n",
    "ho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(ho.shape, out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvNetwork(nn.Module):\n",
    "    '''The Convolution network consists of two Conv1D layers with the \n",
    "    intermediate dimension being named the filter size.'''\n",
    "    def __init__(self, \n",
    "                 ni: int, # Input dimension \n",
    "                 fs: int, # Filter size for intermediate dimension\n",
    "                 ks: list[int]): # A two element array of kernal sizes\n",
    "        ''''''\n",
    "        super().__init__()\n",
    "        assert len(ks) == 2\n",
    "        padding = list(map(lambda x: (x - 1) // 2, ks))\n",
    "        self.layers = nn.ModuleList([Conv1d(ni, fs, ks[0], padding=padding[0]),\n",
    "                                     Conv1d(fs, ni, ks[1], padding=padding[1])])\n",
    "        \n",
    "    def forward(self, inp: tensor):\n",
    "        x = inp.transpose(1,2)\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = ConvNetwork(n_hidden, filter_sz, [9, 1])\n",
    "ho = conv_net(ho)\n",
    "ho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(ho.shape, out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForwardTransformer(nn.Module):\n",
    "    ''''''\n",
    "    def __init__(self, \n",
    "                 ni: int, # Input dimension\n",
    "                 nh: int, # Number of attention heads\n",
    "                 fs: int, # Filter size for intermediate dimension\n",
    "                 ks: list[int], # A two element list of kernal sizes\n",
    "                 p: list[float]): # A two element list of dropout probabilities\n",
    "        '''This module consists of a MultiHeadAttention, and ConvNetwork layer with \n",
    "        dropout, residuals, and layer normalization being applied after each layer'''\n",
    "        super().__init__()\n",
    "        assert len(ks) == 2 and len(p) == 2\n",
    "        self.layers = nn.ModuleList([MultiHeadAttention(ni, nh), ConvNetwork(ni, fs, ks)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(ni) for _ in range(2)])\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(p[i]) for i in range(2)])\n",
    "        \n",
    "    def forward(self, inp: tensor):\n",
    "        res = inp\n",
    "        modules = zip(self.layers, self.norms, self.dropouts)\n",
    "        for layer, norm, dropout in modules:\n",
    "            x = layer(res)\n",
    "            x = norm(dropout(x) + res)\n",
    "            res = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FFTConfig:\n",
    "    '''To allow for easily configurable FFT modules we decided to create a FFTConfig\n",
    "    to allow for more readable, and customizable code when creating FFT'''\n",
    "    def __init__(self, \n",
    "                 ni: int, # The input size\n",
    "                 nh: int, # The number of attention heads\n",
    "                 fs: int, # Filter size for intermediate dimension\n",
    "                 ks: list[int], # A two element list of kernal sizes\n",
    "                 p: list[float]): # A two element list of dropout probabilities\n",
    "        self.ni, self.nh, self.fs, self.ks, self.p = ni, nh, fs, ks, p\n",
    "    \n",
    "    def build(self):\n",
    "        return FeedForwardTransformer(self.ni, self.nh, self.fs, self.ks, self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = FeedForwardTransformer(n_hidden, n_heads, filter_sz, \n",
    "                             ks=[9, 1], p=[0.1, 0.1])\n",
    "ho = fft(inp)\n",
    "ho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(ho.shape, out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duration predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DurationPredictor(nn.Module):\n",
    "    '''This module predicts the logarithmic duration length for each phoneme \n",
    "    based on the phoneme hidden features. It consists of 2-layer 1D convolutional network \n",
    "    with ReLU activation, each followed by the layer normalization and the dropout layer, \n",
    "    and an extra linear layer to output a scalar.'''\n",
    "    def __init__(self, \n",
    "                 ni: int, # Input dimension\n",
    "                 fs: int, # Filter size for intermediate dimension\n",
    "                 ks: list[int], # A two element list of kernal sizes\n",
    "                 p: list[float]): # A two element list of dropout probabilities\n",
    "        ''''''\n",
    "        super().__init__()\n",
    "        assert len(ks) == 2 and len(p) == 2\n",
    "        \n",
    "        padding = list(map(lambda x: (x - 1) // 2, ks))\n",
    "        self.layers = nn.ModuleList([Conv1d(ni, fs, ks[0], padding=padding[0]),\n",
    "                                     Conv1d(fs, ni, ks[1], padding=padding[1])])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(sz) for sz in [fs, ni]])\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(p[i]) for i in range(2)])\n",
    "        self.linear = Linear(ni, 1)\n",
    "    \n",
    "    def forward(self, hi: tensor):\n",
    "        x = hi\n",
    "        modules = zip(self.layers, self.norms, self.dropouts)\n",
    "        for layer, norm, dropout in modules:\n",
    "            x = layer(x.transpose(1, 2))\n",
    "            x = dropout(F.relu(x))\n",
    "            x = norm(x.transpose(1,2))\n",
    "        x = self.linear(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DPConfig:\n",
    "    '''To allow for easily configurable Deration Predictor modules we \n",
    "    decided to create a DPConfig to allow for more readable, \n",
    "    and customizable code when creating Duration Predcitor modules'''\n",
    "    def __init__(self, \n",
    "                 ni: int, # Input dimension\n",
    "                 fs: int, # Filter size for intermediate dimension\n",
    "                 ks: list[int], # A two element list of kernal sizes\n",
    "                 p: list[float]): # A two element list of dropout probabilities\n",
    "        self.ni, self.fs, self.ks, self.p = ni, fs, ks, p\n",
    "    \n",
    "    def build(self):\n",
    "        return DurationPredictor(self.ni, self.fs, self.ks, self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_pred = DurationPredictor(n_hidden, filter_sz, \n",
    "                                  [3,3], [0.1,0.1])\n",
    "log_durations = duration_pred(ho)\n",
    "log_durations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(log_durations.shape, [bs, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length regulator\n",
    "> This module upsamples the phoneme hidden feature to the size of the melspectrogram based on the phoneme durations provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def length_regulator(hi: tensor, # The hidden phoneme features\n",
    "                     durations: tensor, # The phoneme durations to upsample to\n",
    "                     upsample_ratio: float, # The multiplier ratio of upsampling rate\n",
    "                     device: torch.device = None): # Device you want to use\n",
    "    assert len(durations.sum(dim=1).unique()) == 1\n",
    "    durations = (upsample_ratio * durations).to(torch.int)\n",
    "    \n",
    "    (bs, _, nh), sl = hi.shape, durations[0].sum().item()\n",
    "    \n",
    "    ho = torch.zeros((bs, sl, nh), device=device)\n",
    "    for i in range(bs):\n",
    "        ho[i] = hi[i].repeat_interleave(durations[i], dim=0)\n",
    "    return ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = tensor([[10]*seq_len for i in range(bs)])\n",
    "ho = length_regulator(ho, durations, 1.)\n",
    "ho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(ho.shape, [bs, durations[0].sum(), n_hidden])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastspeech\n",
    "> This is module will contain the full architecture for FastSpeech. it will consists of the feed-forward Transformer block, the length regulator, and the duration predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastSpeech(nn.Module):\n",
    "    ''''''\n",
    "    def __init__(self, \n",
    "                 ne: int, # Number of embeddings (vocab size) \n",
    "                 ni: int, # The number of hidden dimension\n",
    "                 no: int, # The number of outputs bins (mel bins)\n",
    "                 ec: FFTConfig, # Encoder config \n",
    "                 enb: int, # The number of FFT in encoder\n",
    "                 dc: FFTConfig, # Decoder config\n",
    "                 dnb: int, #The number of FFT in decoder\n",
    "                 dpc: DPConfig, # Duration Predictor config\n",
    "                 device=None):\n",
    "        ''''''\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(ne, ni)\n",
    "        self.encoder = nn.Sequential(*[ec.build() for _ in range(enb)])\n",
    "        self.decoder = nn.Sequential(*[dc.build() for _ in range(dnb)])\n",
    "        self.duration_predictor = dpc.build()\n",
    "        self.linear = Linear(ni, no)\n",
    "        \n",
    "    def forward(self, inp: tensor, # The input phonemes in vectorized form\n",
    "                durations: tensor = None, # The phoneme durations, used for training\n",
    "                upsample_ratio: float = 1.): # Upsampling ratio (adjust speed of speech)\n",
    "        x = self.embedding(inp)\n",
    "        x = x +  get_positional_embeddings(*x.shape[-2:], device=self.device)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        log_durations = self.duration_predictor(x.detach())\n",
    "        if durations == None or not self.training:\n",
    "            durations = log_durations.exp()\n",
    "        x = length_regulator(x, durations, upsample_ratio, device=self.device)\n",
    "        \n",
    "        x = x +  get_positional_embeddings(*x.shape[-2:], device=self.device)\n",
    "        x = self.decoder(x)\n",
    "        x = self.linear(x).transpose(1,2)\n",
    "        \n",
    "        return (x, log_durations) if self.training else x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = FFTConfig(n_hidden, n_heads, 64, [9,1], [0.1,0.1])\n",
    "decoder_config = FFTConfig(n_hidden, n_heads, 64, [3,3], [0.1,0.1])\n",
    "dp_config = DPConfig(n_hidden, 48, [3,3], [0.1, 0.1])\n",
    "\n",
    "model = FastSpeech(vocab_sz, n_hidden, 80, encoder_config, 4,\n",
    "                   decoder_config, 6, dp_config)\n",
    "with torch.no_grad(): mel, _ = model(sample_batch, durations)\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mel(mel[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
