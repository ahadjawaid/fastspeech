# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_modules.ipynb.

# %% auto 0
__all__ = ['get_positional_embeddings', 'Conv1d', 'Linear', 'scaled_dot_product_attention', 'AttentionHead', 'MultiHeadAttention',
           'ConvNet', 'FeedForwardTransformer', 'DurationPredictor', 'length_regulator', 'PostNet', 'FastSpeech']

# %% ../nbs/01_modules.ipynb 3
import torch
import torch.nn as nn
from torch import tensor
from math import sqrt
import torch.nn.functional as F

# %% ../nbs/01_modules.ipynb 12
def get_positional_embeddings(seq_len, # The length of the sequence
                              d_model, # The hidden dimension of the model
                              device: torch.device =None): # Device you want to use
    pos = torch.arange(d_model, device=device)[None, :]
    i = torch.arange(seq_len, device=device)[:, None]
    angle = pos / torch.pow(10000, 2 * i / d_model)
    pos_emb = torch.zeros(angle.shape, device=device)
    pos_emb[0::2,:], pos_emb[1::2,:] = angle[0::2,:].sin(), angle[1::2,:].cos()
    return pos_emb

# %% ../nbs/01_modules.ipynb 18
class Conv1d(nn.Conv1d):
    ''''''
    def __init__(self, *args, gain="linear", **kwargs):
        ''''''
        super().__init__(*args, **kwargs)
        nn.init.xavier_uniform_(self.weight, nn.init.calculate_gain(gain))

# %% ../nbs/01_modules.ipynb 20
class Linear(nn.Linear):
    ''''''
    def __init__(self, *args, gain="linear", **kwargs):
        ''''''
        super().__init__(*args, **kwargs)
        nn.init.xavier_uniform_(self.weight, nn.init.calculate_gain(gain))

# %% ../nbs/01_modules.ipynb 22
def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = query@key.transpose(1,2) / sqrt(dim_k)
    weights = F.softmax(scores, dim=-1)
    return weights@value

# %% ../nbs/01_modules.ipynb 24
class AttentionHead(nn.Module):
    ''''''
    def __init__(self, embed_dim, head_dim):
        ''''''
        super().__init__()
        self.qkv = nn.Linear(embed_dim, head_dim*3)
    
    def forward(self, hidden_state):
        q, k, v = torch.chunk(self.qkv(hidden_state), 3, dim=-1)
        attn_outputs = scaled_dot_product_attention(q, k, v)
        return attn_outputs

# %% ../nbs/01_modules.ipynb 26
class MultiHeadAttention(nn.Module):
    ''''''
    def __init__(self, config):
        ''''''
        super().__init__()
        embed_dim, num_heads, p = (config["hidden_size"], config["num_attention_heads"],
                                   config["hidden_dropout_prob"])
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) 
                                    for _ in range(num_heads)])
        self.linear = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(p)
    def forward(self, hidden_states):
        x = torch.cat([h(hidden_states) for h in self.heads], dim=-1)
        x = self.linear(x)
        x = self.dropout(x)
        return x

# %% ../nbs/01_modules.ipynb 28
class ConvNet(nn.Module):
    ''''''
    def __init__(self, config):
        ''''''
        super().__init__()
        hs, ks, fs, p = (config["hidden_size"], config["kernal_sizes"], config["filter_size"],
                         config["hidden_dropout_prob"])
        assert len(ks) == 2
        padding = list(map(lambda x: (x - 1) // 2, ks))
        self.conv1 = Conv1d(hs, fs, ks[0], padding=padding[0], gain="relu")
        self.conv2 = Conv1d(fs, hs, ks[1], padding=padding[1], gain="relu")
        self.dropout = nn.Dropout(p)

    def forward(self, x):
        x = self.conv1(x.transpose(1,2))
        x = F.relu(x)
        x = self.conv2(x)
#         x = F.relu(x)
        x = self.dropout(x)
        return x.transpose(1, 2)

# %% ../nbs/01_modules.ipynb 30
class FeedForwardTransformer(nn.Module):
    ''''''
    def __init__(self, config):
        ''''''
        super().__init__()
        self.layer_norm1 = nn.LayerNorm(config["hidden_size"])
        self.layer_norm2 = nn.LayerNorm(config["hidden_size"])
        self.attention = MultiHeadAttention(config["multi_head_attention"])
        self.conv_net = ConvNet(config["conv_net"])
    
    def forward(self, x):
        x = self.layer_norm1(x + self.attention(x))
        x = self.layer_norm1(x + self.conv_net(x))
        return x

# %% ../nbs/01_modules.ipynb 33
class DurationPredictor(nn.Module):
    '''This module predicts the logarithmic duration length for each phoneme 
    based on the phoneme hidden features. It consists of 2-layer 1D convolutional network 
    with ReLU activation, each followed by the layer normalization and the dropout layer, 
    and an extra linear layer to output a scalar.'''
    def __init__(self, config):
        ''''''
        super().__init__()
        hs, ks, fs, p = (config["hidden_size"], config["kernal_sizes"], 
                         config["filter_size"], config["hidden_dropout_prob"])
        assert len(ks) == 2
        
        padding = list(map(lambda x: (x - 1) // 2, ks))
        self.layers = nn.ModuleList([Conv1d(hs, fs, ks[0], padding=padding[0], gain="relu"),
                                     Conv1d(fs, hs, ks[1], padding=padding[1], gain="relu")])
        self.norms = nn.ModuleList([nn.LayerNorm(sz) for sz in [fs, hs]])
        self.dropout = nn.Dropout(p)
        self.linear = Linear(hs, 1)
    
    def forward(self, x):
        modules = zip(self.layers, self.norms)
        for layer, norm in modules:
            x = F.relu(layer(x.transpose(1,2)))
            x = self.dropout(F.relu(x))
            x = norm(x.transpose(1,2))
        x = self.linear(x)
        return x.squeeze(-1)

# %% ../nbs/01_modules.ipynb 36
def length_regulator(hi: tensor, # The hidden phoneme features
                     durations: tensor, # The phoneme durations to upsample to
                     upsample_ratio: float, # The multiplier ratio of upsampling rate
                     device: torch.device = None): # Device you want to use
    assert len(durations.sum(dim=1).unique()) == 1
    durations = (upsample_ratio * durations).to(torch.int)

    (bs, _, nh), sl = hi.shape, durations[0].sum().item()
    
    ho = torch.zeros((bs, sl, nh), device=device)
    for i in range(bs):
        ho[i] = hi[i].repeat_interleave(durations[i], dim=0)
    return ho

# %% ../nbs/01_modules.ipynb 38
class PostNet(nn.Module):
    def __init__(self, config): 
        super().__init__()
        hs, ks, fs, p, nl = (config["hidden_size"], config["kernal_size"], 
                             config["filter_size"], config["hidden_dropout_prob"], config["num_layers"])
        padding = (ks - 1) // 2
        self.layers = nn.ModuleList([Conv1d(hs, fs, ks, padding=padding),
                                   *[Conv1d(fs, fs, ks, padding=padding) 
                                       for i in range(nl-2)],
                                     Conv1d(fs, hs, ks, padding=padding)])
        self.norms = nn.ModuleList([*[nn.BatchNorm1d(fs) for i in range(nl-1)],
                                     nn.BatchNorm1d(hs)])
        self.dropout = nn.Dropout(p)
        
    def forward(self, inp):
        x = inp
        
        modules = zip(self.layers, self.norms)
        for i, (layer, norm) in enumerate(modules):
            x = layer(x)
            x = norm(x)
            x = torch.tanh(x) if i < len(self.layers) - 1 else x
            x = self.dropout(x)
            
        return x

# %% ../nbs/01_modules.ipynb 41
class FastSpeech(nn.Module):
    ''''''
    def __init__(self, config, device=None):
        ''''''
        super().__init__()
        self.device = device
        es, hs, no, ne, nd = (config["embedding_size"], config["hidden_size"], config["num_bins"], 
                              config["num_encoders"], config["num_decoders"])
        self.embedding = nn.Embedding(es, hs)
        self.encoder = nn.Sequential(*[FeedForwardTransformer(config["encoder"]) for _ in range(ne)])
        self.decoder = nn.Sequential(*[FeedForwardTransformer(config["decoder"]) for _ in range(nd)])
        self.duration_predictor = DurationPredictor(config["duration_predictor"])
        self.linear = Linear(hs, no)
#         self.postnet = PostNet(config["postnet"])
        
    def forward(self, phones, durations=None, upsample_ratio=1.):
        x = self.embedding(phones)
        x = x +  get_positional_embeddings(*x.shape[-2:], device=self.device)
        x = self.encoder(x)

        log_durations = self.duration_predictor(x.detach())
        if durations == None or not self.training:
            durations = log_durations.exp()
        x = length_regulator(x, durations, upsample_ratio, device=self.device)
        
        x = x + get_positional_embeddings(*x.shape[-2:], device=self.device)
        x = self.decoder(x)
        x = self.linear(x).transpose(1,2)
        
#         res = self.postnet(x)
#         x = (x + res)
        
#         return (x, log_durations, res) if self.training else x   
        return (x, log_durations) if self.training else x   
