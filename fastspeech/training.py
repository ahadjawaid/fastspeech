# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_training.ipynb.

# %% auto 0
__all__ = ['mae_loss', 'TransformerScheduler', 'FastspeechLearner', 'count_parameters', 'load_checkpoint']

# %% ../nbs/06_training.ipynb 3
from .data import *
import torch
from .visualize import show_mel, plot_loss
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch.nn.utils import clip_grad_value_
from tqdm.auto import tqdm
from torch import tensor
from pathlib import Path

# %% ../nbs/06_training.ipynb 5
def mae_loss(pred: tensor, target: tensor): 
    return torch.abs(target - pred).mean()

# %% ../nbs/06_training.ipynb 7
class TransformerScheduler:
    def __init__(self, optimizer, lr_mul:float, d_model:int, n_warmup_steps:int):
        self._optimizer = optimizer
        self.lr_mul, self.d_model = lr_mul, d_model
        self.n_warmup_steps, self.n_steps = n_warmup_steps, 0

    def step(self):
        self._update_learning_rate()

    def _get_lr_scale(self):
        d_model = self.d_model
        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps
        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))
    
    def _update_learning_rate(self):
        self.n_steps += 1
        lr = self.lr_mul * self._get_lr_scale()

        for param_group in self._optimizer.param_groups:
            param_group['lr'] = lr

# %% ../nbs/06_training.ipynb 8
class FastspeechLearner:
    def __init__(self, dl: DataLoader, norm, Model, Optimizer, Scheduler, 
                config: dict, loss_fn_a, loss_fn_b, loss_fn_c,
                 dl_v: DataLoader = None, accum_grad: int=1, 
                 grad_clip: float=1., fp_16: bool=False, log_interval: int=10, 
                 checkpoint_dir="../checkpoints", device=None):
        
        self.dl, self.norm, self.config = dl, norm, config
        self.dl_v = dl_v
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model = Model(config["model"], device=device).to(device)
        self.optim = Optimizer(self.model.parameters(), **config["optim"])
        self.scheduler = Scheduler(self.optim, **config["scheduler"])
        
        self.loss_fn_a, self.loss_fn_b = loss_fn_a, loss_fn_b
        self.loss_fn_c = loss_fn_c
        
        self.accum_grad, self.grad_clip = accum_grad, grad_clip
        self.fp_16, self.log_interval = fp_16, log_interval
        self.device, self.scaler = device, GradScaler(enabled=self.fp_16)
        
        self.save_num = 0
        self.mel_history = []
        self.loss_history = {"a": [], "b": []}
        self.loss_valid_history = {"a": [], "b": []}
    
    def map_to_device(self, inp: list):
        return list(map(lambda x: x.to(self.device), inp))
    
    def process_batch(self, batch):
        phones, durations, mels = self.map_to_device(batch)
        log_durations = (durations + 1e-20).log()
        return phones, durations, mels, log_durations
        
    def compute_loss(self, phones, durations, mels, log_durations):
        d_slice = (slice(None), slice(None, -1))
        pred_mels, pred_log_durations = self.model(phones, durations)
        loss_a = self.loss_fn_a(pred_mels, mels)
        loss_b =  self.loss_fn_b(pred_log_durations[d_slice], log_durations[d_slice])
        
        return loss_a, loss_b

    def one_step(self, inp: tuple):
        phones, durations, mels, log_durations = self.process_batch(inp)
        self.optim.zero_grad()
        
        with autocast(enabled=self.fp_16):
            loss_a, loss_b = self.compute_loss(phones, durations, mels, 
                                               log_durations)
            loss = loss_a + loss_b
        
        self.scaler.scale(loss).backward()
        
        clip_grad_value_(self.model.parameters(), self.grad_clip)
        
        return loss_a.detach(), loss_b.detach()
    
    def append_valid_loss(self):
        for batch in self.dl_v:
            phones, durations, mels, log_durations = self.process_batch(batch)
            with torch.no_grad():
                loss_a, loss_b = self.compute_loss(phones, durations, mels, 
                                                           log_durations)
            self.loss_valid_history['a'].append(loss_a.cpu())
            self.loss_valid_history['b'].append(loss_b.cpu())
    
    def fit(self, steps: int):
        curr_steps = 0
        
        val_phone, val_duration, val_mel = next(iter(self.dl))
        show_mel(self.norm.denormalize(val_mel)[0], "validation")
              
        progress_bar = tqdm(total=steps, desc="Training", unit="step")
        while curr_steps < steps:
            for batch in self.dl:
                if curr_steps > steps: break
                    
                loss_a, loss_b = self.one_step(batch)
                
                if curr_steps % self.accum_grad == 0 or (curr_steps+1) == len(self.dl):
                    self.scaler.step(self.optim)
                    self.scaler.update()
                    self.scheduler.step()
                
                curr_steps += 1
                self._update_bar(progress_bar, loss_a, loss_b)
                self.loss_history['a'].append(loss_a.cpu())
                self.loss_history['b'].append(loss_b.cpu())
                
                if curr_steps % self.log_interval == 0 or curr_steps == steps:
                    with torch.no_grad(): 
                        val_phone, val_duration = self.map_to_device([val_phone, 
                                                                      val_duration])
                        pred_mel, _ = self.model(val_phone, val_duration)
                        pred_mel = self.norm.denormalize(pred_mel).cpu()
                        
                    self.mel_history.append(pred_mel)
                    
                    loss_valid_text = ''
                    if self.dl_v:
                        self.append_valid_loss()
                        avg_loss_valid = tensor(
                            self.loss_valid_history['a'][-len(self.dl_v):]).mean()
                        loss_valid_text = f", valid loss: {avg_loss_valid:.4f}"
                    
                    if not self.checkpoint_dir.exists():
                        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
                    save_path = self.checkpoint_dir/f"save_{self.save_num}.pt"
                    self.save_num += 1
                    self.save_model(save_path)
        
                    loss_slice = slice(-self.log_interval, None)
                    avg_loss = tensor(self.loss_history['a'][loss_slice]).mean()
                         
                    title = f"step: {curr_steps}, loss: {avg_loss:.4f}{loss_valid_text}"
                    show_mel(pred_mel[0], title)
                    
        progress_bar.close()
        
    def _update_bar(self, progress_bar, loss_a, loss_b):
        progress_bar.update(1)
        loss_output = {"loss_mel": f"{loss_a:.4f}", "loss_durr": f"{loss_b:.2f}"}
        progress_bar.set_postfix(loss_output, refresh=True)
        
    def save_model(self, file_path):
        torch.save({
            "state_dict": {
                'model': self.model.state_dict(),
                'optim': self.optim.state_dict(),
            },
            "config": self.config,
            "norm": self.norm,
        }, file_path)
    
    def load_model_state_dict(self, sd: dict):
        self.model.load_state_dict(sd["model"])
        self.optim.load_state_dict(sd["optim"])

# %% ../nbs/06_training.ipynb 13
def count_parameters(model: torch.nn.Module):
    return  sum([p.numel() for p in model.parameters() if p.requires_grad])

# %% ../nbs/06_training.ipynb 15
def load_checkpoint(file_path: str):
    checkpoint = torch.load(file_path)
    return checkpoint["state_dict"], checkpoint["config"], checkpoint["norm"]
