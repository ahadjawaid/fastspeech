# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_training.ipynb.

# %% auto 0
__all__ = ['mae_loss', 'TransformerScheduler', 'FastspeechLearner', 'count_parameters', 'load_checkpoint']

# %% ../nbs/06_training.ipynb 3
from .data import *
import torch
from .visualize import show_mel, plot_loss
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch.nn.utils import clip_grad_value_
from tqdm.auto import tqdm
from torch import tensor
from pathlib import Path

# %% ../nbs/06_training.ipynb 5
def mae_loss(pred: tensor, target: tensor): 
    return torch.abs(target - pred).mean()

# %% ../nbs/06_training.ipynb 7
class TransformerScheduler:
    def __init__(self, optimizer, lr_mul:float, d_model:int, n_warmup_steps:int):
        self._optimizer = optimizer
        self.lr_mul, self.d_model = lr_mul, d_model
        self.n_warmup_steps, self.n_steps = n_warmup_steps, 0

    def step(self):
        self._update_learning_rate()

    def _get_lr_scale(self):
        d_model = self.d_model
        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps
        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))
    
    def _update_learning_rate(self):
        self.n_steps += 1
        lr = self.lr_mul * self._get_lr_scale()

        for param_group in self._optimizer.param_groups:
            param_group['lr'] = lr

# %% ../nbs/06_training.ipynb 8
class FastspeechLearner:
    def __init__(self, dl: DataLoader, norm, Model, Optimizer, Scheduler, 
                 config: dict, loss_fn_a, loss_fn_b, accum_grad: int=1, 
                 grad_clip: float=1., fp_16: bool=False, log_interval: int=10, 
                 checkpoint_dir="../checkpoints", device=None):
        
        self.dl, self.norm, self.config = dl, norm, config
        self.checkpoint_dir = Path(checkpoint_dir)
        self.model = Model(**config["model"]).to(device)
        self.optim = Optimizer(self.model.parameters(), **config["optim"])
        self.scheduler = Scheduler(self.optim, **config["scheduler"])
        
        self.loss_fn_a, self.loss_fn_b = loss_fn_a, loss_fn_b
        
        self.accum_grad, self.grad_clip = accum_grad, grad_clip
        self.fp_16, self.log_interval = fp_16, log_interval
        self.device, self.scaler = device, GradScaler(enabled=self.fp_16)
        
        self.save_num = 0
        self.mel_history = []
        self.loss_history = []
        
    def one_step(self, inp: tuple):
        phones, durations, mels = map(lambda x: x.to(self.device), inp)
        log_durations = (durations + 1e-20).log()
        d_slice = (slice(None), slice(None, -1))
        self.optim.zero_grad()
        
        with autocast(enabled=self.fp_16):
            pred_mels, pred_log_durations, res = self.model(phones, durations)
            loss_a = self.loss_fn_a(pred_mels, mels)
            loss_b =  self.loss_fn_b(pred_log_durations[d_slice], log_durations[d_slice])
            loss_c = self.loss_fn_a(res, mels)
            total_loss = loss_a + loss_b + loss_c        
        
        self.scaler.scale(total_loss).backward()
        
        clip_grad_value_(self.model.parameters(), self.grad_clip)
        
        return total_loss.detach()
    
    def fit(self, steps: int):
        curr_steps = 0
        
        val_phone, val_duration, val_mel = next(iter(self.dl))
        show_mel(self.norm.denormalize(val_mel)[0], "validation")
              
        progress_bar = tqdm(total=steps, desc="Training", unit="step")
        while curr_steps < steps:
            for batch in self.dl:
                if curr_steps > steps: break
                    
                total_loss = self.one_step(batch)
                
                if curr_steps % self.accum_grad == 0 or (curr_steps+1) == len(self.dl):
                    self.scaler.step(self.optim)
                    self.scaler.update()
                    self.scheduler.step()
                
                curr_steps += 1
                total_loss = total_loss.cpu()
                self._update_bar_and_history(progress_bar, total_loss)
                
                if curr_steps % self.log_interval == 0 or curr_steps == steps:
                    with torch.no_grad(): 
                        pred_mel, _, _ = self.model(val_phone.to(self.device), 
                                                 val_duration.to(self.device))
                        pred_mel = self.norm.denormalize(pred_mel).cpu()
                    
                    if not self.checkpoint_dir.exists():
                        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
                    save_path = self.checkpoint_dir/f"save_{self.save_num}"
                    
                    self.save_model(save_path)
        
                    self.mel_history.append(pred_mel)
            
                    loss_slice = slice(min(0, curr_steps-self.log_interval), curr_steps)
                    avg_loss = tensor(self.loss_history[loss_slice]).mean()
                    
                    title = f"step: {curr_steps}, loss: {avg_loss:.4f}"
                    show_mel(pred_mel[0], title)
                    
        progress_bar.close()
        
    def _update_bar_and_history(self, progress_bar, total_loss):
        self.loss_history.append(total_loss)
        
        progress_bar.update(1)
        loss_output = {"loss": f"{total_loss:.4f}"}
        progress_bar.set_postfix(loss_output, refresh=True)
        
    def save_model(self, file_path):
        torch.save({
            "state_dict": {
                'model': self.model.state_dict(),
                'optim': self.optim.state_dict(),
            },
            "config": self.config,
            "norm": self.norm,
        }, file_path)
    
    def load_model_state_dict(self, sd: dict):
        self.model.load_state_dict(sd["model"])
        self.optim.load_state_dict(sd["optim"])

# %% ../nbs/06_training.ipynb 13
def count_parameters(model: torch.nn.Module):
    return  sum([p.numel() for p in model.parameters() if p.requires_grad])

# %% ../nbs/06_training.ipynb 15
def load_checkpoint(file_path: str):
    checkpoint = torch.load(file_path)
    return checkpoint["state_dict"], checkpoint["config"], checkpoint["norm"]
